{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZTAzxa3Aitslpo3R0n0vl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niyeldeii/Transformers/blob/main/Transformers_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we have to import the required dependencies which in this case is mostly tensorflow and numpy"
      ],
      "metadata": {
        "id": "mC4H_j_c_H7E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wmEsysajvOtu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we implement the Positional Encoding algorithm because transformers process the entire sequence at once unlike RNNs which are sequential"
      ],
      "metadata": {
        "id": "6FT1k5IxDZEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "    def __init__(self, position, d_model):\n",
        "        super().__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angles = self._get_angles(np.arange(position)[:, np.newaxis],\n",
        "                                np.arange(d_model)[np.newaxis, :],\n",
        "                                d_model)\n",
        "\n",
        "        # apply sin to even indices\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        # apply cos to odd indices\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "    def _get_angles(self, pos, i, d_model):\n",
        "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "        return pos * angle_rates\n",
        "\n",
        "    def call(self, x):\n",
        "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n"
      ],
      "metadata": {
        "id": "SEPsh4yNDs0y"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The core of the attention mechanism in the Transformer is the Scaled Dot-Product Attention"
      ],
      "metadata": {
        "id": "8ciZb3daH31h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value):\n",
        "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    # Scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # Softmax is normalized on the last axis (seq_len_k)\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, value)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "wDNolRCrugJN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi Head Attention layer applied multiple attention to different parts of the sequence, and then concatenate the output"
      ],
      "metadata": {
        "id": "jbMm-IJSvZr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention = scaled_dot_product_attention(q, k, v)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "\n",
        "        output = self.dense(concat_attention)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "vwXLWTNdvqE3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's apply the Multi-Head Attention followed by a Feed-Forward Networks, Layer Normalization and Dropout to build the Transformer Block"
      ],
      "metadata": {
        "id": "bdHaZI4tzIbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model)\n",
        "        ])\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training):\n",
        "        # Self Attention\n",
        "        attn_output = self.mha(x, x, x)  # Self attention\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        # Feed Forward Network\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2\n"
      ],
      "metadata": {
        "id": "VpfZYl3Zww2f"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the the full transformer model\n",
        "integrating all the previous components"
      ],
      "metadata": {
        "id": "XJDEUnoFWq1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassifier(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, max_length, d_model, num_heads, dff, num_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(max_length, d_model)\n",
        "\n",
        "        self.transformer_blocks = [\n",
        "            TransformerBlock(d_model, num_heads, dff)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        self.pool = tf.keras.layers.GlobalAveragePooling1D()\n",
        "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "        self.final_layer = tf.keras.layers.Dense(2)  # 2 classes\n",
        "\n",
        "    def call(self, x, training=False):  # Make training default to False\n",
        "        # Embedding\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = self.pos_encoding(x)\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        # Transformer blocks (encoder layers)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, training=training)  # Pass training explicitly\n",
        "\n",
        "        # Pooling and classification\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        return self.final_layer(x)\n"
      ],
      "metadata": {
        "id": "pch6gomiWKtb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's do a simple text classification task for album reviews"
      ],
      "metadata": {
        "id": "GLl-tg4po7UG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data():\n",
        "    text = [\n",
        "        'Solid Album Kendrick is Amazing',\n",
        "        'I can not believe he dropped this trash in 2024',\n",
        "        'Great Album, Album of the Year',\n",
        "        'worst album ever',\n",
        "        'his career is washed without Drake',\n",
        "        'He should have kept this one',\n",
        "        'This is going to be a classic',\n",
        "        'He really is the greatest'\n",
        "    ]\n",
        "    labels = [1, 0, 1, 0, 0, 0, 1, 1]\n",
        "\n",
        "    # Build vocabulary\n",
        "    words = set()\n",
        "    for line in text:\n",
        "        words.update(line.split())\n",
        "    vocab = sorted(words)\n",
        "    word_index = {word: index + 1 for index, word in enumerate(vocab)}\n",
        "\n",
        "    # Determine max_length\n",
        "    max_length = max([len(line.split()) for line in text])\n",
        "\n",
        "    # Convert text to padded sequences\n",
        "    sequences = []\n",
        "    for line in text:\n",
        "        seq = [word_index[word] for word in line.split()]\n",
        "        seq += [0] * (max_length - len(seq))  # Pad sequences with 0s\n",
        "        sequences.append(seq)\n",
        "\n",
        "    # Return sequences, labels, vocab_size, and max_length\n",
        "    return np.array(sequences), np.array(labels), len(vocab) + 1, max_length\n",
        "\n",
        "def train_model():\n",
        "\n",
        "  sequences, labels, vocab_size, max_length = prepare_data()\n",
        "  print(f\"Data shapes: {sequences.shape}, {labels.shape}\")\n",
        "  print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "\n",
        "  d_model = 32\n",
        "  num_heads = 2\n",
        "  dff = 64\n",
        "\n",
        "\n",
        "  model = classifier(\n",
        "        vocab_size=vocab_size,\n",
        "        max_length=max_length,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dff=dff\n",
        "    )\n",
        "\n",
        "    # Compile\n",
        "  model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "  history = model.fit(\n",
        "        sequences,\n",
        "        labels,\n",
        "        epochs=20,\n",
        "        batch_size=2,\n",
        "        validation_split=0.2,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "  return model, history\n",
        "\n",
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "model, history = train_model()\n",
        "\n",
        "# Prediction\n",
        "def predict(text, model):\n",
        "    # Process text similarly to training data\n",
        "    sequence = np.array([[1, 2, 0, 0]])  # Example of padding\n",
        "    prediction = model(sequence)\n",
        "    return prediction\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfjVzmhXpOvH",
        "outputId": "7c7f13de-3cb9-41f5-fe63-4ceed9c07a23"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Data shapes: (8, 10), (8,)\n",
            "Vocabulary size: 42\n",
            "Epoch 1/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 222ms/step - accuracy: 0.3542 - loss: 1.4156 - val_accuracy: 0.0000e+00 - val_loss: 0.8274\n",
            "Epoch 2/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5000 - loss: 0.7338 - val_accuracy: 0.0000e+00 - val_loss: 1.8172\n",
            "Epoch 3/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7083 - loss: 0.8598 - val_accuracy: 0.0000e+00 - val_loss: 1.9652\n",
            "Epoch 4/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5833 - loss: 0.7226 - val_accuracy: 0.0000e+00 - val_loss: 1.6843\n",
            "Epoch 5/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.3750 - loss: 0.8860 - val_accuracy: 0.0000e+00 - val_loss: 1.4708\n",
            "Epoch 6/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8333 - loss: 0.4509 - val_accuracy: 0.0000e+00 - val_loss: 1.3342\n",
            "Epoch 7/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5000 - loss: 0.6657 - val_accuracy: 0.0000e+00 - val_loss: 1.1431\n",
            "Epoch 8/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5000 - loss: 0.7236 - val_accuracy: 0.0000e+00 - val_loss: 1.0197\n",
            "Epoch 9/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7708 - loss: 0.6216 - val_accuracy: 0.0000e+00 - val_loss: 0.8954\n",
            "Epoch 10/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5833 - loss: 0.6036 - val_accuracy: 0.0000e+00 - val_loss: 0.8760\n",
            "Epoch 11/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8542 - loss: 0.5650 - val_accuracy: 0.0000e+00 - val_loss: 0.9241\n",
            "Epoch 12/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6458 - loss: 0.5397 - val_accuracy: 0.0000e+00 - val_loss: 1.0478\n",
            "Epoch 13/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.6458 - loss: 0.5973 - val_accuracy: 0.0000e+00 - val_loss: 1.2047\n",
            "Epoch 14/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6458 - loss: 0.5043 - val_accuracy: 0.0000e+00 - val_loss: 1.3213\n",
            "Epoch 15/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7708 - loss: 0.4323 - val_accuracy: 0.0000e+00 - val_loss: 1.3741\n",
            "Epoch 16/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 1.0000 - loss: 0.2974 - val_accuracy: 0.0000e+00 - val_loss: 1.3869\n",
            "Epoch 17/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7292 - loss: 0.4167 - val_accuracy: 0.0000e+00 - val_loss: 1.3432\n",
            "Epoch 18/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 0.2753 - val_accuracy: 0.0000e+00 - val_loss: 1.3312\n",
            "Epoch 19/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.1829 - val_accuracy: 0.0000e+00 - val_loss: 1.4279\n",
            "Epoch 20/20\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0888 - val_accuracy: 0.0000e+00 - val_loss: 1.7464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, word_index, max_length):\n",
        "    # Tokenize and convert text to a sequence of integers\n",
        "    sequence = [word_index.get(word, 0) for word in text.split()]\n",
        "    # Pad the sequence to match the max_length\n",
        "    sequence += [0] * (max_length - len(sequence))\n",
        "    return np.array([sequence])  # Return as a batch\n",
        "\n",
        "def predict_sentiment(text, model, word_index, max_length):\n",
        "    # Preprocess the text\n",
        "    sequence = preprocess_text(text, word_index, max_length)\n",
        "    # Get the model prediction\n",
        "    logits = model(sequence, training=False)  # Inference mode\n",
        "    probabilities = tf.nn.softmax(logits).numpy()\n",
        "    predicted_class = np.argmax(probabilities, axis=-1)[0]\n",
        "    confidence = probabilities[0, predicted_class]\n",
        "\n",
        "    return predicted_class, confidence\n",
        "\n",
        "# Prepare the vocabulary and maximum length\n",
        "sequences, labels, vocab_size, max_length = prepare_data()\n",
        "\n",
        "# Word index is created in prepare_data, replicate here\n",
        "text = [\n",
        "    'Solid Album Kendrick is Amazing',\n",
        "    'I can not believe he dropped this trash in 2024',\n",
        "    'Great Album, Album of the Year',\n",
        "    'worst album ever',\n",
        "    'his career is washed without Drake',\n",
        "    'He should have kept this one',\n",
        "    'This is going to be a classic',\n",
        "    'He really is the greatest'\n",
        "]\n",
        "words = set()\n",
        "for line in text:\n",
        "    words.update(line.split())\n",
        "vocab = sorted(words)\n",
        "word_index = {word: index + 1 for index, word in enumerate(vocab)}\n",
        "\n",
        "# Example text to predict\n",
        "new_text = \" masterpiece\"\n",
        "predicted_class, confidence = predict_sentiment(new_text, model, word_index, max_length)\n",
        "\n",
        "print(f\"Predicted Class: {'Positive' if predicted_class == 1 else 'Negative'}, Confidence: {confidence:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxrS2gNgyFOF",
        "outputId": "fe4418ec-93c7-46e8-a608-7c7645df2c79"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: Negative, Confidence: 0.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DULyQQ4E0yaw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}